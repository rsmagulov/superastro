# app/knowledge/cli.py
from __future__ import annotations

import argparse
import hashlib
import json
import re
import sqlite3
import sys
import zipfile
import xml.etree.ElementTree as ET
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Optional

VALIDATOR_PROFILE = "ru_v1"

# ----------------------------
# Paths / time
# ----------------------------


def _astro_root() -> Path:
    # .../astroprocessor/app/knowledge/cli.py -> astroprocessor root = parents[2]
    return Path(__file__).resolve().parents[2]


def _staging_db_path() -> Path:
    p = _astro_root() / "data" / "staging" / "staging.db"
    p.parent.mkdir(parents=True, exist_ok=True)
    return p


def _prod_db_path() -> Path:
    p = _astro_root() / "data" / "knowledge.db"
    p.parent.mkdir(parents=True, exist_ok=True)
    return p


def _utcnow_iso() -> str:
    return datetime.now(timezone.utc).replace(microsecond=0).isoformat().replace("+00:00", "Z")


def _die(msg: str, code: int = 2) -> int:
    print(f"‚ùå {msg}")
    return code


def _connect(path: Path) -> sqlite3.Connection:
    conn = sqlite3.connect(str(path))
    conn.row_factory = sqlite3.Row
    return conn


def _sha256_file(path: Path) -> str:
    h = hashlib.sha256()
    with path.open("rb") as f:
        for chunk in iter(lambda: f.read(1024 * 1024), b""):
            h.update(chunk)
    return h.hexdigest()


def _json_or_empty(s: str | None) -> str:
    s = (s or "").strip()
    if not s:
        return "{}"
    try:
        json.loads(s)
        return s
    except Exception:
        return "{}"


def _json_load_dict(s: str | None) -> dict[str, Any]:
    """Best-effort JSON dict loader. Returns {} on any problem."""
    raw = (s or "").strip()
    if not raw:
        return {}
    try:
        obj = json.loads(raw)
    except Exception:
        return {}
    return obj if isinstance(obj, dict) else {}


def _merge_meta_json(existing: str | None, updates: dict[str, Any]) -> str:
    """Merge updates into existing meta_json, preserving other fields."""
    obj = _json_load_dict(existing)
    obj.update(updates)
    return json.dumps(obj, ensure_ascii=False)


def _is_managed_by_build(meta_json: str | None) -> bool:
    """Return True if prod meta_json marks the row as created by kb build.

    We treat a row as "managed" only if meta_json contains a positive integer
    field: kb_fragment_id.
    """
    obj = _json_load_dict(meta_json)
    v = obj.get("kb_fragment_id")
    return isinstance(v, int) and v > 0


def _table_exists(conn: sqlite3.Connection, name: str) -> bool:
    row = conn.execute(
        "SELECT 1 FROM sqlite_master WHERE type IN ('table','view') AND name=? LIMIT 1",
        (name,),
    ).fetchone()
    return bool(row)


# ----------------------------
# Staging schema
# ----------------------------

STAGING_SCHEMA_VERSION = 2

STAGING_SCHEMA_SQL = """
CREATE TABLE IF NOT EXISTS kb_schema_meta (
  key TEXT PRIMARY KEY,
  value TEXT NOT NULL
);

CREATE TABLE IF NOT EXISTS kb_sources (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  title TEXT NOT NULL,
  path TEXT,
  sha256 TEXT,
  imported_at TEXT NOT NULL,
  notes TEXT
);

CREATE INDEX IF NOT EXISTS idx_kb_sources_sha256 ON kb_sources(sha256);

CREATE TABLE IF NOT EXISTS kb_fragments (
  id INTEGER PRIMARY KEY AUTOINCREMENT,

  -- —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ –≤ —Ä–∞–º–∫–∞—Ö —è–∑—ã–∫–∞
  key TEXT NOT NULL,
  locale TEXT NOT NULL DEFAULT 'ru-RU',

  topic_category TEXT NOT NULL,

  text TEXT NOT NULL,

  tone TEXT NOT NULL CHECK (tone IN ('supportive','neutral','warning')),
  abstraction_level TEXT NOT NULL CHECK (abstraction_level IN ('psychological','symbolic','behavioral')),

  state TEXT NOT NULL CHECK (state IN (
    'draft','needs_review','reviewed','annotated','validated','enabled','archived'
  )),

  -- –ø—Ä–∏–≤—è–∑–∫–∏ –∫ –∞—Å—Ç—Ä–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º —Ñ–∞–∫—Ç–æ—Ä–∞–º (–ø–ª–∞–Ω–µ—Ç–∞/–¥–æ–º/–∞—Å–ø–µ–∫—Ç –∏ —Ç.–ø.)
  factors_json TEXT NOT NULL DEFAULT '{}',

  -- –º–µ—Ç–∞/—Ç—ç–≥–∏/–¥–æ–ø. –ø–æ–ª—è (–Ω–∞ –±—É–¥—É—â–µ–µ)
  meta_json TEXT NOT NULL DEFAULT '{}',

  source_id INTEGER,
  author TEXT,

  created_at TEXT NOT NULL,
  updated_at TEXT NOT NULL,

  FOREIGN KEY (source_id) REFERENCES kb_sources(id) ON DELETE SET NULL,
  UNIQUE (key, locale)
);

CREATE INDEX IF NOT EXISTS idx_kb_fragments_state ON kb_fragments(state);
CREATE INDEX IF NOT EXISTS idx_kb_fragments_topic ON kb_fragments(topic_category);
CREATE INDEX IF NOT EXISTS idx_kb_fragments_locale ON kb_fragments(locale);
CREATE INDEX IF NOT EXISTS idx_kb_fragments_key ON kb_fragments(key);

CREATE TABLE IF NOT EXISTS kb_events (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  fragment_id INTEGER NOT NULL,
  from_state TEXT,
  to_state TEXT NOT NULL,
  who TEXT,
  note TEXT,
  ts TEXT NOT NULL,
  FOREIGN KEY (fragment_id) REFERENCES kb_fragments(id) ON DELETE CASCADE
);

CREATE INDEX IF NOT EXISTS idx_kb_events_fragment ON kb_events(fragment_id);
CREATE INDEX IF NOT EXISTS idx_kb_events_ts ON kb_events(ts);

CREATE TABLE IF NOT EXISTS kb_builds (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  version TEXT NOT NULL UNIQUE,           -- v0001, v0002, ...
  created_at TEXT NOT NULL,
  manifest_json TEXT NOT NULL DEFAULT '{}',
  stats_json TEXT NOT NULL DEFAULT '{}'
);
""".strip()

STAGING_SCHEMA_V2_SQL = """
-- v2 additions: richer sources + chunks + ingest runs

CREATE TABLE IF NOT EXISTS kb_source_chunks (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  source_id INTEGER NOT NULL,
  seq INTEGER NOT NULL,
  locale TEXT NOT NULL DEFAULT 'ru-RU',
  text TEXT NOT NULL,
  sha256 TEXT NOT NULL,
  char_start INTEGER NOT NULL DEFAULT 0,
  char_end INTEGER NOT NULL DEFAULT 0,
  created_at TEXT NOT NULL,
  UNIQUE(source_id, seq)
);

CREATE INDEX IF NOT EXISTS idx_kb_source_chunks_source_seq ON kb_source_chunks(source_id, seq);
CREATE INDEX IF NOT EXISTS idx_kb_source_chunks_sha256 ON kb_source_chunks(sha256);

CREATE TABLE IF NOT EXISTS kb_ingest_runs (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  started_at TEXT NOT NULL,
  finished_at TEXT,
  input_path TEXT,
  tool TEXT NOT NULL,
  tool_version TEXT NOT NULL,
  files_total INTEGER NOT NULL DEFAULT 0,
  sources_created INTEGER NOT NULL DEFAULT 0,
  sources_reused INTEGER NOT NULL DEFAULT 0,
  chunks_created INTEGER NOT NULL DEFAULT 0,
  errors_json TEXT NOT NULL DEFAULT '{}'
);
""".strip()


def _has_column(conn: sqlite3.Connection, table: str, column: str) -> bool:
    cur = conn.execute(f"PRAGMA table_info({table})")
    return any(r[1] == column for r in cur.fetchall())


def _migrate_staging_v2(conn: sqlite3.Connection) -> None:
    """Staging schema v2 migration: enrich kb_sources and add chunk/run tables."""
    if _table_exists(conn, "kb_sources"):
        if not _has_column(conn, "kb_sources", "source_type"):
            conn.execute("ALTER TABLE kb_sources ADD COLUMN source_type TEXT")
        if not _has_column(conn, "kb_sources", "author"):
            conn.execute("ALTER TABLE kb_sources ADD COLUMN author TEXT")
        if not _has_column(conn, "kb_sources", "locale"):
            conn.execute("ALTER TABLE kb_sources ADD COLUMN locale TEXT")
        if not _has_column(conn, "kb_sources", "file_name"):
            conn.execute("ALTER TABLE kb_sources ADD COLUMN file_name TEXT")
        if not _has_column(conn, "kb_sources", "file_size"):
            conn.execute("ALTER TABLE kb_sources ADD COLUMN file_size INTEGER")
        if not _has_column(conn, "kb_sources", "meta_json"):
            conn.execute("ALTER TABLE kb_sources ADD COLUMN meta_json TEXT NOT NULL DEFAULT '{}'")

    conn.executescript(STAGING_SCHEMA_V2_SQL)


def _ensure_staging_schema(conn: sqlite3.Connection) -> None:
    """Ensure staging schema exists and is migrated forward."""
    if not _table_exists(conn, "kb_fragments"):
        conn.executescript(STAGING_SCHEMA_SQL)
        _migrate_staging_v2(conn)
        conn.execute(
            "INSERT OR REPLACE INTO kb_schema_meta(key,value) VALUES(?,?)",
            ("schema_version", str(STAGING_SCHEMA_VERSION)),
        )
        conn.execute(
            "INSERT OR REPLACE INTO kb_schema_meta(key,value) VALUES(?,?)",
            ("initialized_at", _utcnow_iso()),
        )
        conn.commit()
        return

    cur = conn.execute("SELECT value FROM kb_schema_meta WHERE key='schema_version'")
    row = cur.fetchone()
    version = int(row[0]) if row and str(row[0]).isdigit() else 1

    if version < 2:
        _migrate_staging_v2(conn)
        conn.execute(
            "INSERT OR REPLACE INTO kb_schema_meta(key,value) VALUES(?,?)",
            ("schema_version", "2"),
        )
        conn.commit()


# ----------------------------
# Production schema (minimal)
# ----------------------------

PROD_SCHEMA_SQL = """
CREATE TABLE IF NOT EXISTS kb_meta (
  key TEXT PRIMARY KEY,
  value TEXT NOT NULL
);

CREATE TABLE IF NOT EXISTS knowledge_docs (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  title TEXT NOT NULL,
  path TEXT,
  sha256 TEXT,
  imported_at TEXT,
  meta_json TEXT NOT NULL DEFAULT '{}'
);

CREATE TABLE IF NOT EXISTS knowledge_chunks (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  doc_id INTEGER NOT NULL,
  chunk_index INTEGER NOT NULL,
  text TEXT NOT NULL,
  meta_json TEXT NOT NULL DEFAULT '{}',
  FOREIGN KEY (doc_id) REFERENCES knowledge_docs(id) ON DELETE CASCADE
);

CREATE TABLE IF NOT EXISTS knowledge_items (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  key TEXT NOT NULL,
  topic_category TEXT,
  locale TEXT NOT NULL DEFAULT 'ru-RU',
  text TEXT,
  priority INTEGER NOT NULL DEFAULT 0,
  created_at TEXT NOT NULL,
  is_active INTEGER NOT NULL DEFAULT 0,
  meta_json TEXT NOT NULL DEFAULT '{}'
);

CREATE INDEX IF NOT EXISTS idx_knowledge_items_key ON knowledge_items(key);
CREATE INDEX IF NOT EXISTS idx_knowledge_items_locale ON knowledge_items(locale);
CREATE INDEX IF NOT EXISTS idx_knowledge_items_topic ON knowledge_items(topic_category);
CREATE INDEX IF NOT EXISTS idx_knowledge_items_active ON knowledge_items(is_active);
""".strip()


def _ensure_prod_schema(conn: sqlite3.Connection) -> None:
    if not _table_exists(conn, "knowledge_items"):
        conn.executescript(PROD_SCHEMA_SQL)
        conn.commit()


# ----------------------------
# Helpers: printing
# ----------------------------


def _print_rows(rows: list[sqlite3.Row]) -> None:
    for r in rows:
        print(
            f"{int(r['id']):<8} {str(r['key']):<36} {str(r['topic_category']):<22} "
            f"{str(r['locale']):<7} {str(r['tone']):<12} {str(r['abstraction_level']):<14} {str(r['state']):<10}"
        )
        print(f"        {str(r['created_at'])}")


def _print_prod_rows(rows: list[sqlite3.Row], *, with_text: bool = False, text_limit: int = 120) -> None:
    for r in rows:
        keys = set(r.keys())

        def _v(name: str, default: Any = "") -> Any:
            return r[name] if name in keys else default

        print(
            f"{int(_v('id', 0)):<6} {str(_v('key', '')):<36} {str(_v('topic_category', '') or ''):<20} "
            f"{str(_v('locale', '') or ''):<7} {int(_v('is_active', 0) or 0):<2} {int(_v('priority', 0) or 0):<6} "
            f"{str(_v('created_at', '') or '')}"
        )

        if with_text:
            t = str(_v("text", "") or "")
            t = t.replace("\r", " ").replace("\n", " ").strip()
            if len(t) > text_limit:
                t = t[:text_limit] + "‚Ä¶"
            if t:
                print("       " + t)


# ----------------------------
# Commands
# ----------------------------


def cmd_init(args: argparse.Namespace) -> int:
    sp = _staging_db_path()
    conn = _connect(sp)
    try:
        conn.executescript(STAGING_SCHEMA_SQL)
        _migrate_staging_v2(conn)
        conn.execute(
            "INSERT OR REPLACE INTO kb_schema_meta(key,value) VALUES(?,?)",
            ("schema_version", str(STAGING_SCHEMA_VERSION)),
        )
        conn.execute(
            "INSERT OR REPLACE INTO kb_schema_meta(key,value) VALUES(?,?)",
            ("initialized_at", _utcnow_iso()),
        )
        conn.commit()
        print(f"‚úÖ staging db initialized: {sp}")
    finally:
        conn.close()

    prod = _prod_db_path()
    pc = _connect(prod)
    try:
        pc.executescript(PROD_SCHEMA_SQL)
        pc.commit()
    finally:
        pc.close()

    return 0


def cmd_add(args: argparse.Namespace) -> int:
    conn = _connect(_staging_db_path())
    try:
        _ensure_staging_schema(conn)

        now = _utcnow_iso()

        key = (args.key or "").strip()
        if not key:
            return _die("key is required")

        locale = (args.locale or "ru-RU").strip() or "ru-RU"
        topic = (args.topic or "").strip()
        if not topic:
            return _die("topic is required (--topic)")

        text_ = (args.text or "")
        tone = (args.tone or "").strip()
        abstraction = (args.abstraction or "").strip()
        state = (args.state or "draft").strip()
        author = (args.author or "").strip() or None

        factors_json = _json_or_empty(args.factors_json)
        meta_json = _json_or_empty(args.meta_json)

        source_id = None
        if args.source_title:
            title = str(args.source_title).strip()
            if title:
                imported_at = now
                notes = (args.source_notes or "").strip() or None
                path = (args.source_path or "").strip() or None

                sha = None
                if path:
                    p = Path(path)
                    if p.exists() and p.is_file():
                        sha = _sha256_file(p)

                row = conn.execute("SELECT id FROM kb_sources WHERE title=?", (title,)).fetchone()
                if row:
                    source_id = int(row["id"])
                else:
                    cur = conn.execute(
                        "INSERT INTO kb_sources(title,path,sha256,imported_at,notes) VALUES(?,?,?,?,?)",
                        (title, path, sha, imported_at, notes),
                    )
                    source_id = int(cur.lastrowid)

        cur = conn.execute(
            """
            INSERT INTO kb_fragments(
              key, locale, topic_category, text, tone, abstraction_level, state,
              factors_json, meta_json, source_id, author, created_at, updated_at
            )
            VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?)
            """.strip(),
            (
                key,
                locale,
                topic,
                text_,
                tone,
                abstraction,
                state,
                factors_json,
                meta_json,
                source_id,
                author,
                now,
                now,
            ),
        )
        fid = int(cur.lastrowid)

        conn.execute(
            """
            INSERT INTO kb_events(fragment_id, from_state, to_state, who, note, ts)
            VALUES(?,?,?,?,?,?)
            """.strip(),
            (fid, None, state, (args.who or "KB").strip(), "created", now),
        )

        conn.commit()
        print(f"‚úÖ added: id={fid} key={key} state={state} locale={locale} topic={topic}")
        return 0
    finally:
        conn.close()


def cmd_state(args: argparse.Namespace) -> int:
    key = (args.key or "").strip()
    if not key:
        return _die("key is required (--key)")

    locale = (args.locale or "ru-RU").strip() or "ru-RU"
    to_state = (args.to or "").strip()
    if not to_state:
        return _die("target state is required (--to)")

    who = (args.who or "KB").strip()
    note = (args.note or "").strip() or None

    conn = _connect(_staging_db_path())
    try:
        _ensure_staging_schema(conn)

        row = conn.execute(
            "SELECT id, state FROM kb_fragments WHERE key=? AND locale=?",
            (key, locale),
        ).fetchone()
        if not row:
            return _die("not found", 2)

        fragment_id = int(row["id"])
        from_state = str(row["state"] or "").strip()

        # Gate: –Ω–µ–ª—å–∑—è –≤–∫–ª—é—á–∞—Ç—å –±–µ–∑ validated
        if to_state == "enabled" and from_state not in {"validated", "enabled"}:
            print(
                "‚ùå cannot enable: fragment must be in 'validated' state first (run: kb validate ...)",
                file=sys.stderr,
            )
            return 3

        if from_state == to_state:
            print(f"‚ÑπÔ∏è state unchanged: {key} already {to_state}", file=sys.stderr)
            return 0

        now = _utcnow_iso()
        conn.execute(
            "UPDATE kb_fragments SET state=?, updated_at=? WHERE id=?",
            (to_state, now, fragment_id),
        )
        conn.execute(
            """
            INSERT INTO kb_events(fragment_id, from_state, to_state, who, note, ts)
            VALUES(?,?,?,?,?,?)
            """.strip(),
            (fragment_id, from_state, to_state, who, note, now),
        )
        conn.commit()

        # –í–ê–ñ–ù–û: stdout –æ—Å—Ç–∞–≤–ª—è–µ–º —á–∏—Å—Ç—ã–º (–¥–ª—è –ø–∞–π–ø–æ–≤/JSON), –ø–æ—ç—Ç–æ–º—É –≤—Å—ë ‚Äî –≤ stderr.
        print(f"‚úÖ state: key={key} locale={locale} {from_state} -> {to_state}", file=sys.stderr)
        return 0
    finally:
        conn.close()


def _validate_fragment_row(row: sqlite3.Row) -> list[str]:
    errs: list[str] = []

    def _g(col: str) -> Any:
        try:
            return row[col]
        except Exception:
            return None

    key = str(_g("key") or "").strip()
    if not key:
        errs.append("key is empty")
    elif any(ch.isspace() for ch in key):
        errs.append("key contains whitespace")

    topic = str(_g("topic_category") or "").strip()
    if not topic:
        errs.append("topic_category is empty")

    locale = str(_g("locale") or "ru-RU").strip() or "ru-RU"

    text_raw = str(_g("text") or "")
    text_stripped = text_raw.strip()
    if not text_stripped:
        errs.append("text is empty")
    else:
        min_len = 60
        max_len = 2000
        if len(text_stripped) < min_len:
            errs.append(f"text too short (<{min_len} chars)")
        if len(text_stripped) > max_len:
            errs.append(f"text too long (>{max_len} chars)")

        lines = [ln.strip() for ln in text_stripped.splitlines() if ln.strip()]
        first_line = lines[0] if lines else ""
        first_line_alpha = re.sub(r"[^A-Za-z–ê-–Ø–∞-—è–Å—ë]", "", first_line)

        if first_line and first_line_alpha:
            is_all_caps = first_line_alpha.upper() == first_line_alpha
            if is_all_caps and (len(first_line_alpha) >= 8) and (len(lines) >= 2):
                errs.append("starts with ALL-CAPS headline")

        if text_raw.count("\n") >= 8:
            errs.append("too many newlines (>=8)")
        if re.search(r"\n\s*\n\s*\n", text_raw):
            errs.append("contains 3+ consecutive blank lines")

        bad_markers = [
            "lorem ipsum",
            "placeholder",
            "todo",
            "tbd",
            "qwe",
            "asdf",
            "test test",
            "–∑–∞–≥–ª—É—à–∫–∞",
            "–º—É—Å–æ—Ä",
        ]
        low = text_stripped.lower()
        if any(b in low for b in bad_markers):
            errs.append("contains placeholder/garbage marker")

        if "```" in text_raw:
            errs.append("contains code fence ```")
        if "http://" in low or "https://" in low:
            errs.append("contains URL")

        if locale.lower().startswith("ru"):
            address_markers = [
                "—Ç—ã",
                "—Ç–µ–±–µ",
                "—Ç–≤–æ–π",
                "—Ç–≤–æ—ë",
                "—Ç–≤–æ–µ–º",
                "—É —Ç–µ–±—è",
                "–≤—ã",
                "–≤–∞–º",
                "–≤–∞—à",
                "—É –≤–∞—Å",
            ]
            ru_garbage_markers = [
                "–±–∏–æ–∞—Å—Ç—Ä–æ–ª–æ–≥–∏—è",
                "–±–∞–Ω–∏—Ç—å",
                "—Ü–µ–ø–ª—è—Ç—å",
                "–∫–∞—Ä—Ç—É –Ω–∞ –∫—É—Å–∫–∏",
                "—Å–º—ã—Ç—å –ø–µ—Ä—Ö–æ—Ç—å",
                "–¥—Ä–µ–≤–Ω–∏—Ö –∑–∞–±–ª—É–∂–¥–µ–Ω–∏–π",
                "–≤–Ω–µ—Å—Ç–∏ —Å–≤–æ—é —Å–∫—Ä–æ–º–Ω—É—é –ª–µ–ø—Ç—É",
                "–ø—Ä–æ–≥—Ä–µ—Å—Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –Ω–∞—É–∫–∏",
            ]
            if any(m in low for m in ru_garbage_markers):
                errs.append("ru: contains meta/technical/garbage phrasing")
            has_address = any(m in low for m in address_markers)
            if not has_address:
                errs.append("ru: missing direct address (—Ç—ã/–≤—ã/–≤–∞–º/—É —Ç–µ–±—è/—É –≤–∞—Å)")
            if not re.search(r"[.!?‚Ä¶]", text_stripped):
                errs.append("ru: missing sentence punctuation")

    tone = str(_g("tone") or "").strip()
    if tone not in ("supportive", "neutral", "warning"):
        errs.append(f"invalid tone: {tone!r}")

    ab = str(_g("abstraction_level") or "").strip()
    if ab not in ("psychological", "symbolic", "behavioral"):
        errs.append(f"invalid abstraction_level: {ab!r}")

    for col in ("factors_json", "meta_json"):
        raw = str(_g(col) or "").strip() or "{}"
        try:
            obj = json.loads(raw)
            if not isinstance(obj, dict):
                errs.append(f"{col} must be a JSON object")
        except Exception as e:
            errs.append(f"{col} invalid JSON: {type(e).__name__}: {e}")

    return errs


def cmd_validate(args: argparse.Namespace) -> int:
    conn = _connect(_staging_db_path())
    try:
        _ensure_staging_schema(conn)

        key = (args.key or "").strip()
        locale_arg = args.locale
        state_arg = args.state
        recheck = bool(getattr(args, "recheck", False))
        as_json = bool(getattr(args, "json", False))
        verbose = bool(getattr(args, "verbose", False))
        silent = recheck and as_json and not verbose

        def log(msg: str) -> None:
            if not silent:
                print(msg, file=sys.stderr)

        params: dict[str, Any] = {}
        where: list[str] = []

        if key:
            where.append("key = :key")
            params["key"] = key

            if locale_arg:
                where.append("locale = :loc")
                params["loc"] = str(locale_arg).strip()

            if state_arg:
                where.append("state = :state")
                params["state"] = str(state_arg).strip()
        else:
            locale = str(locale_arg or "ru-RU").strip()
            state = str(state_arg or "draft").strip()
            where.append("locale = :loc")
            params["loc"] = locale
            where.append("state = :state")
            params["state"] = state

        rows = conn.execute(
            f"""
            SELECT id, key, locale, topic_category, text, tone, abstraction_level, state,
                   factors_json, meta_json
            FROM kb_fragments
            WHERE {" AND ".join(where) if where else "1=1"}
            ORDER BY id ASC
            LIMIT :limit
            """.strip(),
            {**params, "limit": int(args.limit or 1000)},
        ).fetchall()

        if not rows:
            if as_json:
                print(json.dumps({"checked": 0, "ok": 0, "bad": 0, "updated": 0, "items": []}, ensure_ascii=False))
            else:
                log("‚ÑπÔ∏è nothing to validate")
            return 0

        ok_ids: list[int] = []
        bad_items: list[dict[str, Any]] = []
        ok_items: list[dict[str, Any]] = []

        for r in rows:
            errs = _validate_fragment_row(r)
            item = {
                "id": int(r["id"]),
                "key": str(r["key"]),
                "locale": str(r["locale"]),
                "state": str(r["state"]),
                "ok": not bool(errs),
                "errors": errs,
            }
            if errs:
                bad_items.append(item)
            else:
                ok_items.append(item)
                ok_ids.append(int(r["id"]))

        if recheck:
            if as_json:
                print(
                    json.dumps(
                        {
                            "validator_profile": VALIDATOR_PROFILE_RU_V1,
                            "checked": len(rows),
                            "ok": len(ok_items),
                            "bad": len(bad_items),
                            "updated": 0,
                            "items": ok_items + bad_items,
                        },
                        ensure_ascii=False,
                        indent=2,
                    )
                )
            else:
                if bad_items:
                    log("‚ùå recheck failed:")
                    for it in bad_items:
                        log(f"  - {it['key']}: " + "; ".join(it["errors"]))
                log(f"‚úÖ recheck: checked={len(rows)} ok={len(ok_items)} bad={len(bad_items)}")
            return 0 if not bad_items else 1

        if getattr(args, "strict", False) and bad_items:
            if as_json:
                print(
                    json.dumps(
                        {
                            "validator_profile": VALIDATOR_PROFILE,
                            "checked": len(rows),
                            "ok": len(ok_items),
                            "bad": len(bad_items),
                            "updated": 0,
                            "items": ok_items + bad_items,
                        },
                        ensure_ascii=False,
                        indent=2,
                    )
                )
            return 2

        updated = 0
        now = _utcnow_iso()
        who = (args.who or "KB").strip()
        note = (args.note or "validated").strip()

        for fid in ok_ids:
            row = conn.execute("SELECT state, meta_json FROM kb_fragments WHERE id=?", (fid,)).fetchone()
            if not row:
                continue

            from_state = str(row["state"] or "").strip()

            if from_state in {"validated", "enabled", "archived"}:
                continue

            row2 = conn.execute(
                "SELECT meta_json FROM kb_fragments WHERE id=?",
                (fid,),
            ).fetchone()
            meta = _json_load_dict(row2["meta_json"] if row2 else None)
            meta["validator_profile"] = VALIDATOR_PROFILE_RU_V1

            cur = conn.execute(
                "UPDATE kb_fragments SET state='validated', meta_json=?, updated_at=? WHERE id=?",
                (json.dumps(meta, ensure_ascii=False), now, fid),
            )

            if cur.rowcount:
                updated += int(cur.rowcount)
                conn.execute(
                    """
                    INSERT INTO kb_events(fragment_id, from_state, to_state, who, note, ts)
                    VALUES(?,?,?,?,?,?)
                    """.strip(),
                    (fid, from_state, "validated", who, note, now),
                )

        conn.commit()

        if as_json:
            print(
                json.dumps(
                    {
                        "validator_profile": VALIDATOR_PROFILE,
                        "checked": len(rows),
                        "ok": len(ok_items),
                        "bad": len(bad_items),
                        "updated": updated,
                        "items": ok_items + bad_items,
                    },
                    ensure_ascii=False,
                    indent=2,
                )
            )
        else:
            log(f"‚úÖ validate ok: checked={len(rows)} ok={len(ok_items)} updated={updated} bad={len(bad_items)}")

        return 0 if not bad_items else 1
    finally:
        conn.close()


def cmd_build(args: argparse.Namespace) -> int:
    dry_run = bool(getattr(args, "dry_run", False))
    as_json = bool(getattr(args, "json", False))
    compact = bool(getattr(args, "compact", False))

    staging = _connect(_staging_db_path())
    prod = _connect(_prod_db_path())
    try:
        _ensure_staging_schema(staging)
        _ensure_prod_schema(prod)

        if dry_run:
            prod.execute("BEGIN")

        enabled_rows = staging.execute(
            """
            SELECT id, key, locale, topic_category, text, tone, abstraction_level, source_id, author, created_at, meta_json
            FROM kb_fragments
            WHERE state='enabled'
            ORDER BY id ASC
            """.strip()
        ).fetchall()

        enabled = len(enabled_rows)
        inserted = 0
        updated = 0
        deactivated = 0
        skipped_not_managed = 0

        inserted_pairs: list[tuple[str, str]] = []
        updated_pairs: list[tuple[str, str]] = []
        deactivated_pairs: list[tuple[str, str]] = []

        for r in enabled_rows:
            key = str(r["key"])
            locale = str(r["locale"])
            topic = str(r["topic_category"])
            text_ = str(r["text"] or "")
            created_at = str(r["created_at"] or _utcnow_iso())

            st_meta = _json_load_dict(r["meta_json"])
            meta = {
                "tone": str(r["tone"]),
                "abstraction_level": str(r["abstraction_level"]),
                "validator_profile": st_meta.get("validator_profile"),
                "kb_fragment_id": int(r["id"]),
                "source_id": (int(r["source_id"]) if r["source_id"] is not None else None),
                "author": (str(r["author"]) if r["author"] is not None else None),
            }
            meta_json = json.dumps(meta, ensure_ascii=False)

            cur = prod.execute(
                """
                UPDATE knowledge_items
                SET topic_category=?, text=?, priority=?, is_active=?, meta_json=?
                WHERE key=? AND locale=?
                """.strip(),
                (topic, text_, 200, 1, meta_json, key, locale),
            )
            if int(cur.rowcount or 0) > 0:
                updated += int(cur.rowcount or 0)
                updated_pairs.append((key, locale))
                continue

            prod.execute(
                """
                INSERT INTO knowledge_items(key, topic_category, locale, text, priority, created_at, is_active, meta_json)
                VALUES(?,?,?,?,?,?,?,?)
                """.strip(),
                (key, topic, locale, text_, 200, created_at, 1, meta_json),
            )
            inserted += 1
            inserted_pairs.append((key, locale))

        managed_pairs = staging.execute("SELECT DISTINCT key, locale FROM kb_fragments").fetchall()
        managed_set = {(str(r["key"]), str(r["locale"])) for r in managed_pairs}

        enabled_pairs = staging.execute(
            "SELECT DISTINCT key, locale FROM kb_fragments WHERE state='enabled'"
        ).fetchall()
        enabled_set = {(str(r["key"]), str(r["locale"])) for r in enabled_pairs}

        to_deactivate = sorted(managed_set - enabled_set)
        for key, locale in to_deactivate:
            row = prod.execute(
                "SELECT id, is_active, meta_json FROM knowledge_items WHERE key=? AND locale=?",
                (key, locale),
            ).fetchone()
            if not row:
                continue

            if not _is_managed_by_build(row["meta_json"]):
                skipped_not_managed += 1
                continue

            cur = prod.execute(
                "UPDATE knowledge_items SET is_active=0 WHERE id=? AND is_active<>0",
                (int(row["id"]),),
            )
            n = int(cur.rowcount or 0)
            if n > 0:
                deactivated += n
                deactivated_pairs.append((key, locale))

        if dry_run:
            prod.execute("ROLLBACK")
        else:
            prod.execute("CREATE TABLE IF NOT EXISTS kb_meta (key TEXT PRIMARY KEY, value TEXT NOT NULL)")
            prod.execute(
                "INSERT OR REPLACE INTO kb_meta(key,value) VALUES(?,?)",
                ("updated_at", datetime.now().replace(microsecond=0).isoformat(timespec="seconds")),
            )
            prod.execute("INSERT OR REPLACE INTO kb_meta(key,value) VALUES(?,?)", ("enabled", str(enabled)))
            prod.execute("INSERT OR REPLACE INTO kb_meta(key,value) VALUES(?,?)", ("inserted", str(inserted)))
            prod.execute("INSERT OR REPLACE INTO kb_meta(key,value) VALUES(?,?)", ("updated", str(updated)))
            prod.execute("INSERT OR REPLACE INTO kb_meta(key,value) VALUES(?,?)", ("deactivated", str(deactivated)))
            prod.commit()

        report = {
            "dry_run": dry_run,
            "enabled": enabled,
            "inserted": inserted,
            "updated": updated,
            "deactivated": deactivated,
            "skipped_not_managed": skipped_not_managed,
            "validator_profile": VALIDATOR_PROFILE,
            "inserted_items": [{"key": k, "locale": loc} for k, loc in inserted_pairs],
            "updated_items": [{"key": k, "locale": loc} for k, loc in updated_pairs],
            "deactivated_items": [{"key": k, "locale": loc} for k, loc in deactivated_pairs],
            "prod_path": str(_prod_db_path()),
        }

        if as_json:
            if compact:
                print(json.dumps(report, ensure_ascii=False, separators=(",", ":")))
            else:
                print(json.dumps(report, ensure_ascii=False, indent=2))
            return 0

        prefix = "üß™ dry-run" if dry_run else "‚úÖ build ok"
        print(
            f"{prefix}: enabled={enabled} inserted={inserted} updated={updated} deactivated={deactivated}\n"
            f"   prod: {_prod_db_path()}"
        )
        return 0
    finally:
        staging.close()
        prod.close()


def cmd_list(args: argparse.Namespace) -> int:
    key = (getattr(args, "key", "") or "").strip()
    state = (args.state or "").strip()
    topic = (args.topic or "").strip()
    locale = (args.locale or "").strip()
    q = (args.q or "").strip()
    limit = int(args.limit or 50)
    source_id = int(getattr(args, "source_id", 0) or 0)

    if getattr(args, "prod", False):
        conn = _connect(_prod_db_path())
        try:
            _ensure_prod_schema(conn)

            where = ["1=1"]
            params: list[Any] = []

            if getattr(args, "active", False):
                where.append("is_active=1")
            if getattr(args, "inactive", False):
                where.append("is_active=0")
            if key:
                where.append("key = ?")
                params.append(key)

            if topic:
                where.append("topic_category = ?")
                params.append(topic)
            if locale:
                where.append("locale = ?")
                params.append(locale)
            if q:
                where.append("(key LIKE ? OR text LIKE ?)")
                params.extend([f"%{q}%", f"%{q}%"])

            sql = f"""
                SELECT id, key, topic_category, locale, is_active, priority, created_at, text
                FROM knowledge_items
                WHERE {' AND '.join(where)}
                ORDER BY priority DESC, id ASC
                LIMIT ?
            """.strip()
            params.append(limit)

            rows = conn.execute(sql, params).fetchall()

            needs_rewrite_only = bool(getattr(args, "needs_rewrite_only", False))
            bot_ready_only = bool(getattr(args, "bot_ready_only", False))

            if needs_rewrite_only or bot_ready_only:
                filtered: list[sqlite3.Row] = []
                for r in rows:
                    meta = _json_load_dict(r["meta_json"])
                    requires_rewrite = bool(meta.get("requires_rewrite", False))
                    has_addr = bool(_DIRECT_ADDRESS_RE.search(str(r["text"] or "")))

                    if needs_rewrite_only and not requires_rewrite:
                        continue
                    if bot_ready_only and not has_addr:
                        continue
                    filtered.append(r)
                rows = filtered

            _print_rows(rows)
            return 0
        finally:
            conn.close()

    conn = _connect(_staging_db_path())
    try:
        _ensure_staging_schema(conn)

        where = ["1=1"]
        params = []
        if key:
            where.append("key = ?")
            params.append(key)
        if state:
            where.append("state = ?")
            params.append(state)
        if source_id:
            where.append("source_id = ?")
            params.append(source_id)
        if topic:
            where.append("topic_category = ?")
            params.append(topic)
        if locale:
            where.append("locale = ?")
            params.append(locale)
        if q:
            where.append("(key LIKE ? OR text LIKE ?)")
            params.extend([f"%{q}%", f"%{q}%"])

        sql = f"""
            SELECT id, key, topic_category, locale, tone, abstraction_level, state, created_at, meta_json, text
            FROM kb_fragments
            WHERE {' AND '.join(where)}
            ORDER BY id DESC
            LIMIT ?
        """.strip()
        params.append(limit)

        rows = conn.execute(sql, params).fetchall()
        _print_rows(rows)
        return 0
    finally:
        conn.close()


def cmd_show(args: argparse.Namespace) -> int:
    key = (args.key or "").strip()
    if not key:
        raise SystemExit("kb show: --key is required")

    with_text = bool(getattr(args, "with_text", False))
    as_json = bool(getattr(args, "json", False))

    staging = _connect(_staging_db_path())
    prod = _connect(_prod_db_path())
    try:
        _ensure_staging_schema(staging)
        _ensure_prod_schema(prod)

        st_rows = staging.execute(
            """
            SELECT id, key, state, topic_category, locale, tone, abstraction_level, created_at, updated_at,
                   source_id, author, factors_json, meta_json, text
            FROM kb_fragments
            WHERE key=?
            ORDER BY id DESC
            LIMIT 20
            """.strip(),
            (key,),
        ).fetchall()

        pr_rows = prod.execute(
            """
            SELECT id, key, topic_category, locale, is_active, priority, created_at, meta_json, text
            FROM knowledge_items
            WHERE key=?
            ORDER BY id DESC
            LIMIT 20
            """.strip(),
            (key,),
        ).fetchall()

        if as_json:
            payload = {"key": key, "staging": [dict(r) for r in st_rows], "production": [dict(r) for r in pr_rows]}
            print(json.dumps(payload, ensure_ascii=False, indent=2))
            return 0

        if not st_rows and not pr_rows:
            print("‚ÑπÔ∏è not found in staging or production")
            return 0

        if st_rows:
            print("=== STAGING (kb_fragments) ===")
            for r in st_rows:
                print(
                    f"- id={r['id']} state={r['state']} locale={r['locale']} topic={r['topic_category']} "
                    f"tone={r['tone']} ab={r['abstraction_level']} created={r['created_at']} updated={r['updated_at']}"
                )
                print(f"  source_id={r['source_id']} author={r['author']}")
                print(f"  factors_json={r['factors_json']}")
                print(f"  meta_json={r['meta_json']}")
                if with_text:
                    print("  text:")
                    print(str(r["text"] or ""))
                print()

        if pr_rows:
            print("=== PRODUCTION (knowledge_items) ===")
            for r in pr_rows:
                print(
                    f"- id={r['id']} active={r['is_active']} locale={r['locale']} topic={r['topic_category']} "
                    f"priority={r['priority']} created={r['created_at']}"
                )
                print(f"  meta_json={r['meta_json']}")
                if with_text:
                    print("  text:")
                    print(str(r["text"] or ""))
                print()

        return 0
    finally:
        staging.close()
        prod.close()


def cmd_restore(args: argparse.Namespace) -> int:
    key = (args.key or "").strip()
    if not key:
        raise SystemExit("kb restore: --key is required")

    locale_filter = (getattr(args, "locale", None) or "").strip()
    who = (getattr(args, "who", "KB") or "KB").strip()
    note = (getattr(args, "note", "restore") or "restore").strip()
    review_state = (getattr(args, "review_state", "needs_review") or "needs_review").strip()
    dry_run = bool(getattr(args, "dry_run", False))
    run_build = bool(getattr(args, "build", False))
    build_json = bool(getattr(args, "json", False))
    build_compact = bool(getattr(args, "compact", False))
    verbose = bool(getattr(args, "verbose", False))

    silent = run_build and build_json and not verbose

    def log(msg: str) -> None:
        if not silent:
            print(msg, file=sys.stderr)

    conn = _connect(_staging_db_path())
    try:
        _ensure_staging_schema(conn)

        if locale_filter:
            row = conn.execute(
                """
                SELECT *
                FROM kb_fragments
                WHERE key=? AND locale=?
                ORDER BY id DESC
                LIMIT 1
                """.strip(),
                (key, locale_filter),
            ).fetchone()
        else:
            row = conn.execute(
                """
                SELECT *
                FROM kb_fragments
                WHERE key=?
                ORDER BY id DESC
                LIMIT 1
                """.strip(),
                (key,),
            ).fetchone()

        if not row:
            print("‚ÑπÔ∏è not found in staging")
            return 0

        fid = int(row["id"])
        locale = str(row["locale"])
        state0 = str(row["state"] or "").strip()
        now = _utcnow_iso()
        sim_state = state0

        def _event(from_state: str, to_state: str, _note: str) -> None:
            if dry_run:
                return
            conn.execute(
                """
                INSERT INTO kb_events(fragment_id, from_state, to_state, who, note, ts)
                VALUES(?,?,?,?,?,?)
                """.strip(),
                (fid, from_state, to_state, who, _note, now),
            )

        def _set_state(to_state: str, _note: str) -> str:
            nonlocal sim_state

            if dry_run:
                from_state = sim_state
                log(f"üß™ dry-run: {from_state} -> {to_state} ({_note})")
                sim_state = to_state
                return to_state

            cur_state_row = conn.execute("SELECT state FROM kb_fragments WHERE id=?", (fid,)).fetchone()
            if not cur_state_row:
                raise RuntimeError("fragment disappeared during restore")

            from_state = str(cur_state_row["state"] or "").strip()

            if to_state == "validated":
                meta_row = conn.execute("SELECT meta_json FROM kb_fragments WHERE id=?", (fid,)).fetchone()
                conn.execute(
                    "UPDATE kb_fragments SET state=?, updated_at=?, meta_json=? WHERE id=?",
                    (
                        to_state,
                        now,
                        _merge_meta_json((meta_row["meta_json"] if meta_row else None), {"validator_profile": VALIDATOR_PROFILE}),
                        fid,
                    ),
                )
            else:
                conn.execute("UPDATE kb_fragments SET state=?, updated_at=? WHERE id=?", (to_state, now, fid))
            _event(from_state, to_state, _note)
            sim_state = to_state
            return to_state

        if state0 == "enabled":
            log(f"‚úÖ restore: already enabled key={key} locale={locale}")
            if run_build:
                cmd_build(argparse.Namespace(dry_run=dry_run, json=build_json, compact=build_compact))
            return 0

        if state0 == "archived":
            _set_state(review_state, f"{note}: unarchive")

        cur = conn.execute("SELECT * FROM kb_fragments WHERE id=?", (fid,)).fetchone()
        if not cur:
            print("‚ÑπÔ∏è not found in staging")
            return 0

        errs = _validate_fragment_row(cur)
        if errs:
            print("‚ùå restore blocked by validation:")
            print("  - " + "; ".join(errs))
            return 1

        if sim_state != "validated":
            if sim_state in {"enabled", "archived"}:
                log(f"‚ùå restore: cannot validate from terminal state={sim_state!r}")
                return 2
            _set_state("validated", f"{note}: validated")

        if sim_state != "validated":
            print("‚ùå restore: invariant failed; expected state='validated' before enabling")
            return 2

        _set_state("enabled", f"{note}: enabled")
        if not dry_run:
            conn.commit()
        log(f"‚úÖ restore ok: key={key} locale={locale} -> enabled")

        if run_build:
            cmd_build(argparse.Namespace(dry_run=dry_run, json=build_json, compact=build_compact))
        return 0
    finally:
        conn.close()


# ----------------------------
# Ingest / Atomize (FB2)
# ----------------------------

_DIRECT_ADDRESS_RE = re.compile(
    r"\b(—Ç—ã|—Ç–µ–±–µ|—Ç–≤–æ–π|—Ç–≤–æ—è|—Ç–≤–æ–∏|–≤–∞—Å|–≤–∞–º|–≤–∞—à|–≤–∞—à–∞|–≤–∞—à–∏|—É\s+—Ç–µ–±—è|—É\s+–≤–∞—Å)\b", re.IGNORECASE
)
_DOMAIN_RE = re.compile(
    r"(?i)\b(?:[a-z0-9](?:[a-z0-9-]{0,61}[a-z0-9])?\.)+(?:ru|com|net|org|pro|io|mg|tv|me|info|biz)(?:/[^\s<>()\"']*)?"
)

VALIDATOR_PROFILE_RU_V1 = "ru_v1"

def _sha256_bytes(data: bytes) -> str:
    h = hashlib.sha256()
    h.update(data)
    return h.hexdigest()


def _sha1_text(s: str) -> str:
    h = hashlib.sha1()
    h.update(s.encode("utf-8"))
    return h.hexdigest()


def _read_bytes(path: Path) -> bytes:
    return path.read_bytes()


def _extract_fb2_from_zip(zip_path: Path) -> bytes:
    with zipfile.ZipFile(zip_path, "r") as zf:
        names = [n for n in zf.namelist() if n.lower().endswith(".fb2")]
        if not names:
            raise ValueError("zip does not contain .fb2")
        with zf.open(names[0], "r") as fp:
            return fp.read()


def _normalize_text(s: str) -> str:
    s = (s or "").replace("\r\n", "\n").replace("\r", "\n")
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n{3,}", "\n\n", s)
    return s.strip()

_BOX_GARBLE_RE = re.compile(r"[\u2500-\u259f\u00a4]")  # box drawing + currency sign


def _garble_score(s: str) -> float:
    s = s or ""
    if not s:
        return 0.0
    bad = len(_BOX_GARBLE_RE.findall(s))
    return bad / max(1, len(s))


def _fb2_to_text(fb2_bytes: bytes) -> tuple[str, str]:
    """Return (locale, plain_text) from FB2 bytes (best-effort, with encoding recovery)."""

    def parse_bytes(data: bytes) -> tuple[str, str]:
        root = ET.fromstring(data)

        ns = ""
        if root.tag.startswith("{"):
            ns = root.tag.split("}")[0] + "}"

        def _t(el: Any) -> str:
            return ("".join(el.itertext()) if el is not None else "").strip()

        lang_el = root.find(f".//{ns}description/{ns}title-info/{ns}lang")
        lang = _t(lang_el) or "ru-RU"
        locale = "ru-RU" if lang.lower().startswith("ru") else lang

        paragraphs: list[str] = []
        for p in root.findall(f".//{ns}body//{ns}p"):
            s = _t(p)
            if s:
                paragraphs.append(s)

        text = "\n\n".join(paragraphs)
        return locale, _normalize_text(text)

    # 1) Try as-is (ET will use XML prolog encoding if present)
    try:
        locale, text = parse_bytes(fb2_bytes)
        if _garble_score(text) < 0.002:  # –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω–µ—Ç box-—Å–∏–º–≤–æ–ª–æ–≤
            return locale, text
    except Exception:
        locale, text = "ru-RU", ""

    # 2) Encoding recovery: decode bytes with candidates and reparse as Unicode XML
    #    (ET.fromstring(str) treats it as already-decoded text).
    candidates = ["windows-1251", "cp1251", "koi8-r", "cp866"]
    best_locale = locale or "ru-RU"
    best_text = text or ""
    best_score = _garble_score(best_text) if best_text else 1.0

    for enc in candidates:
        try:
            xml_text = fb2_bytes.decode(enc, errors="strict")
        except Exception:
            continue

        # Remove/neutralize wrong encoding declaration if any, to avoid confusing downstream tools.
        # ET will parse unicode anyway, but keep it clean.
        xml_text2 = re.sub(
            r'(<\?xml[^>]*encoding\s*=\s*["\'])([^"\']+)(["\'])',
            r"\1utf-8\3",
            xml_text,
            flags=re.IGNORECASE,
        )

        try:
            root = ET.fromstring(xml_text2)  # unicode input
        except Exception:
            continue

        ns = ""
        if root.tag.startswith("{"):
            ns = root.tag.split("}")[0] + "}"

        def _t(el: Any) -> str:
            return ("".join(el.itertext()) if el is not None else "").strip()

        lang_el = root.find(f".//{ns}description/{ns}title-info/{ns}lang")
        lang = _t(lang_el) or "ru-RU"
        loc = "ru-RU" if lang.lower().startswith("ru") else lang

        paragraphs: list[str] = []
        for p in root.findall(f".//{ns}body//{ns}p"):
            s = _t(p)
            if s:
                paragraphs.append(s)

        txt = _normalize_text("\n\n".join(paragraphs))
        score = _garble_score(txt)

        if txt and score < best_score:
            best_score = score
            best_text = txt
            best_locale = loc

        if best_score < 0.002:
            break

    return best_locale, best_text


def _chunk_text(text: str, chunk_size: int) -> list[tuple[int, int, str]]:
    if not text:
        return []
    paras = [p.strip() for p in text.split("\n\n") if p.strip()]
    chunks: list[tuple[int, int, str]] = []
    buf: list[str] = []
    start = 0
    pos = 0

    def flush() -> None:
        nonlocal buf, start, pos
        if not buf:
            return
        chunk = "\n\n".join(buf).strip()
        end = start + len(chunk)
        chunks.append((start, end, chunk))
        pos = end + 2
        start = pos
        buf = []

    for p in paras:
        if not buf:
            start = pos
        if sum(len(x) for x in buf) + (2 * len(buf)) + len(p) > chunk_size and buf:
            flush()
        buf.append(p)
        pos += len(p) + 2
    flush()
    return chunks


def cmd_sources_list(args: argparse.Namespace) -> int:
    conn = _connect(_staging_db_path())
    try:
        _ensure_staging_schema(conn)
        limit = int(getattr(args, "limit", 200) or 200)
        q = (getattr(args, "q", "") or "").strip()

        where = ["1=1"]
        params: list[Any] = []
        if q:
            where.append("(title LIKE ? OR path LIKE ? OR file_name LIKE ?)")
            like = f"%{q}%"
            params.extend([like, like, like])

        sql = f"""
        SELECT
          s.id, s.title, s.source_type, s.locale, s.file_name, s.file_size, s.imported_at,
          (SELECT COUNT(1) FROM kb_source_chunks c WHERE c.source_id=s.id) AS chunks_count,
          (SELECT COUNT(1) FROM kb_fragments f WHERE f.source_id=s.id) AS fragments_count
        FROM kb_sources s
        WHERE {' AND '.join(where)}
        ORDER BY s.id DESC
        LIMIT ?
        """
        params.append(limit)
        rows = conn.execute(sql, params).fetchall()

        items = []
        for r in rows:
            items.append(
                {
                    "id": int(r["id"]),
                    "title": r["title"],
                    "source_type": r["source_type"] if "source_type" in r.keys() else None,
                    "locale": r["locale"] if "locale" in r.keys() else None,
                    "file_name": r["file_name"] if "file_name" in r.keys() else None,
                    "file_size": r["file_size"] if "file_size" in r.keys() else None,
                    "imported_at": r["imported_at"],
                    "chunks": int(r["chunks_count"] or 0),
                    "fragments": int(r["fragments_count"] or 0),
                }
            )

        if getattr(args, "json", False):
            print(
                json.dumps(
                    {"count": len(items), "items": items},
                    ensure_ascii=False,
                    indent=None if getattr(args, "compact", False) else 2,
                )
            )
        else:
            for it in items:
                print(
                    f"source#{it['id']} chunks={it['chunks']} frags={it['fragments']} "
                    f"type={it.get('source_type') or ''} locale={it.get('locale') or ''} title={it['title']}",
                    file=sys.stderr,
                )
        return 0
    finally:
        conn.close()

def cmd_sources_diagnose(args: argparse.Namespace) -> int:
    source_id = int(getattr(args, "source_id", 0) or 0)
    if not source_id:
        print("ERROR: --source-id is required", file=sys.stderr)
        return 2

    threshold = float(getattr(args, "threshold", 0.01) or 0.01)
    top = int(getattr(args, "top", 10) or 10)
    as_json = bool(getattr(args, "json", False))
    compact = bool(getattr(args, "compact", False))

    conn = _connect(_staging_db_path())
    try:
        _ensure_staging_schema(conn)

        src = conn.execute(
            "SELECT id, title, source_type, locale, file_name, file_size, imported_at FROM kb_sources WHERE id=?",
            (source_id,),
        ).fetchone()
        if not src:
            print("ERROR: source not found", file=sys.stderr)
            return 2

        rows = conn.execute(
            "SELECT id, seq, locale, text FROM kb_source_chunks WHERE source_id=? ORDER BY seq ASC",
            (source_id,),
        ).fetchall()

        scores: list[tuple[int, float, str]] = []
        for r in rows:
            txt = str(r["text"] or "")
            score = _garble_score(txt)
            preview = re.sub(r"\s+", " ", txt.strip())[:160]
            scores.append((int(r["seq"]), float(score), preview))

        chunks_total = len(scores)
        if chunks_total == 0:
            payload = {
                "source": {
                    "id": int(src["id"]),
                    "title": src["title"],
                    "source_type": src["source_type"] if "source_type" in src.keys() else None,
                    "locale": src["locale"] if "locale" in src.keys() else None,
                    "file_name": src["file_name"] if "file_name" in src.keys() else None,
                    "file_size": src["file_size"] if "file_size" in src.keys() else None,
                    "imported_at": src["imported_at"],
                },
                "chunks_total": 0,
                "threshold": threshold,
                "chunks_garbled": 0,
                "score_min": 0.0,
                "score_avg": 0.0,
                "score_max": 0.0,
                "worst": [],
            }
            if as_json:
                print(json.dumps(payload, ensure_ascii=False, indent=None if compact else 2))
            else:
                print(f"diagnose: source_id={source_id} chunks_total=0", file=sys.stderr)
            return 0

        score_min = min(s for _, s, _ in scores)
        score_max = max(s for _, s, _ in scores)
        score_avg = sum(s for _, s, _ in scores) / chunks_total
        chunks_garbled = sum(1 for _, s, _ in scores if s >= threshold)

        worst = sorted(scores, key=lambda x: x[1], reverse=True)[:top]
        worst_items = [{"seq": seq, "garble_score": sc, "preview": prev} for seq, sc, prev in worst]

        payload = {
            "source": {
                "id": int(src["id"]),
                "title": src["title"],
                "source_type": src["source_type"] if "source_type" in src.keys() else None,
                "locale": src["locale"] if "locale" in src.keys() else None,
                "file_name": src["file_name"] if "file_name" in src.keys() else None,
                "file_size": src["file_size"] if "file_size" in src.keys() else None,
                "imported_at": src["imported_at"],
            },
            "chunks_total": chunks_total,
            "threshold": threshold,
            "chunks_garbled": chunks_garbled,
            "score_min": score_min,
            "score_avg": score_avg,
            "score_max": score_max,
            "worst": worst_items,
        }

        if as_json:
            print(json.dumps(payload, ensure_ascii=False, indent=None if compact else 2))
        else:
            print(
                f"diagnose: source_id={source_id} chunks_total={chunks_total} "
                f"garbled={chunks_garbled} threshold={threshold} "
                f"min={score_min:.4f} avg={score_avg:.4f} max={score_max:.4f}",
                file=sys.stderr,
            )
            for it in worst_items:
                print(f"  - seq={it['seq']} score={it['garble_score']:.4f} {it['preview']}", file=sys.stderr)

        return 0
    finally:
        conn.close()

def cmd_fragments_sample(args: argparse.Namespace) -> int:
    source_id = int(getattr(args, "source_id", 0) or 0)
    if not source_id:
        print("ERROR: --source-id is required", file=sys.stderr)
        return 2

    limit = int(getattr(args, "limit", 20) or 20)
    with_text = bool(getattr(args, "with_text", False))
    needs_rewrite_only = bool(getattr(args, "needs_rewrite_only", False))
    bot_ready_only = bool(getattr(args, "bot_ready_only", False))
    state = (getattr(args, "state", "") or "").strip()
    as_json = bool(getattr(args, "json", False))
    compact = bool(getattr(args, "compact", False))

    conn = _connect(_staging_db_path())
    try:
        _ensure_staging_schema(conn)

        where = ["source_id=?"]
        params: list[Any] = [source_id]
        if state:
            where.append("state=?")
            params.append(state)

        rows = conn.execute(
            f"""
            SELECT id, key, locale, topic_category, tone, abstraction_level, state, created_at, meta_json, text
            FROM kb_fragments
            WHERE {' AND '.join(where)}
            ORDER BY id DESC
            LIMIT ?
            """.strip(),
            [*params, limit * 10],
        ).fetchall()

        items: list[dict[str, Any]] = []
        for r in rows:
            meta = _json_load_dict(r["meta_json"])
            requires_rewrite = bool(meta.get("requires_rewrite", False))
            txt = str(r["text"] or "")
            has_addr = bool(_DIRECT_ADDRESS_RE.search(txt))
            has_url = ("http://" in txt.lower()) or ("https://" in txt.lower())

            if needs_rewrite_only and not requires_rewrite:
                continue
            if bot_ready_only and not has_addr:
                continue

            item = {
                "id": int(r["id"]),
                "key": str(r["key"]),
                "locale": str(r["locale"]),
                "state": str(r["state"]),
                "requires_rewrite": requires_rewrite,
                "bot_ready": has_addr,
                "has_url": has_url,
            }
            if with_text:
                item["text"] = txt
            items.append(item)
            if len(items) >= limit:
                break

        if as_json:
            print(json.dumps({"source_id": source_id, "count": len(items), "items": items},
                             ensure_ascii=False, indent=None if compact else 2))
            return 0

        for it in items:
            flags = []
            if it["requires_rewrite"]:
                flags.append("needs_rewrite")
            if it["bot_ready"]:
                flags.append("bot_ready")
            if it["has_url"]:
                flags.append("has_url")
            print(f"{it['id']:<8} {it['key']:<36} {it['locale']:<7} {it['state']:<12} "
                  f"{(','.join(flags) if flags else '-')}",
                  file=sys.stderr)
            if with_text:
                print(it.get("text", ""), file=sys.stderr)
                print("-" * 60, file=sys.stderr)

        return 0
    finally:
        conn.close()

def cmd_fragments_promote(args: argparse.Namespace) -> int:
    source_id = int(getattr(args, "source_id", 0) or 0)
    if not source_id:
        print("ERROR: --source-id is required", file=sys.stderr)
        return 2

    from_state = (getattr(args, "from_state", "needs_review") or "needs_review").strip()
    to_state = (getattr(args, "to_state", "reviewed") or "reviewed").strip()
    only_bot_ready = bool(getattr(args, "only_bot_ready", False))
    only_needs_rewrite = bool(getattr(args, "only_needs_rewrite", False))
    dry_run = bool(getattr(args, "dry_run", False))
    as_json = bool(getattr(args, "json", False))
    compact = bool(getattr(args, "compact", False))
    limit = int(getattr(args, "limit", 0) or 0)
    who = (getattr(args, "who", "KB") or "KB").strip()
    note = (getattr(args, "note", "promote") or "promote").strip()

    if from_state == to_state:
        print("ERROR: from_state equals to_state", file=sys.stderr)
        return 2

    conn = _connect(_staging_db_path())
    try:
        _ensure_staging_schema(conn)

        rows = conn.execute(
            """
            SELECT id, key, locale, state, meta_json, text
            FROM kb_fragments
            WHERE source_id=? AND state=?
            ORDER BY id ASC
            """.strip(),
            (source_id, from_state),
        ).fetchall()

        eligible_ids: list[int] = []
        skipped_filter = 0

        for r in rows:
            meta = _json_load_dict(r["meta_json"])
            requires_rewrite = bool(meta.get("requires_rewrite", False))
            txt = str(r["text"] or "")
            bot_ready = bool(_DIRECT_ADDRESS_RE.search(txt))

            if only_bot_ready and not bot_ready:
                skipped_filter += 1
                continue
            if only_needs_rewrite and not requires_rewrite:
                skipped_filter += 1
                continue

            eligible_ids.append(int(r["id"]))
            if limit and len(eligible_ids) >= limit:
                break

        updated = 0
        now = _utcnow_iso()

        if not dry_run:
            for fid in eligible_ids:
                conn.execute(
                    "UPDATE kb_fragments SET state=?, updated_at=? WHERE id=?",
                    (to_state, now, fid),
                )
                conn.execute(
                    """
                    INSERT INTO kb_events(fragment_id, from_state, to_state, who, note, ts)
                    VALUES(?,?,?,?,?,?)
                    """.strip(),
                    (fid, from_state, to_state, who, note, now),
                )
                updated += 1
            conn.commit()
        else:
            updated = len(eligible_ids)

        payload = {
            "dry_run": dry_run,
            "source_id": source_id,
            "from_state": from_state,
            "to_state": to_state,
            "only_bot_ready": only_bot_ready,
            "only_needs_rewrite": only_needs_rewrite,
            "matched": len(rows),
            "eligible": len(eligible_ids),
            "updated": updated,
            "skipped_filter": skipped_filter,
        }

        if as_json:
            print(json.dumps(payload, ensure_ascii=False, indent=None if compact else 2))
        else:
            print(
                f"promote: source_id={source_id} {from_state}->{to_state} "
                f"matched={len(rows)} eligible={len(eligible_ids)} updated={updated} skipped_filter={skipped_filter}",
                file=sys.stderr,
            )
        return 0
    finally:
        conn.close()


def cmd_fragments_stats(args: argparse.Namespace) -> int:
    source_id = int(getattr(args, "source_id", 0) or 0)
    if not source_id:
        print("ERROR: --source-id is required", file=sys.stderr)
        return 2

    state = (getattr(args, "state", "") or "").strip()
    as_json = bool(getattr(args, "json", False))
    compact = bool(getattr(args, "compact", False))

    conn = _connect(_staging_db_path())
    try:
        _ensure_staging_schema(conn)

        where = ["source_id=?"]
        params: list[Any] = [source_id]
        if state:
            where.append("state=?")
            params.append(state)

        rows = conn.execute(
            f"SELECT meta_json, text FROM kb_fragments WHERE {' AND '.join(where)}",
            params,
        ).fetchall()

        total = len(rows)
        requires_rewrite = 0
        bot_ready = 0
        has_url = 0
        has_domain = 0

        for r in rows:
            meta = _json_load_dict(r["meta_json"])
            if bool(meta.get("requires_rewrite", False)):
                requires_rewrite += 1
            txt = str(r["text"] or "")
            if _DIRECT_ADDRESS_RE.search(txt):
                bot_ready += 1
            low = txt.lower()
            if "http://" in low or "https://" in low:
                has_url += 1
            if _DOMAIN_RE.search(txt):
                has_domain += 1

        payload = {
            "source_id": source_id,
            "state": state or None,
            "total": total,
            "requires_rewrite": requires_rewrite,
            "bot_ready": bot_ready,
            "has_url": has_url,
            "has_domain": has_domain,
        }

        if as_json:
            print(json.dumps(payload, ensure_ascii=False, indent=None if compact else 2))
        else:
            print(
                f"source_id={source_id} total={total} requires_rewrite={requires_rewrite} "
                f"bot_ready={bot_ready} has_url={has_url}",
                file=sys.stderr,
            )
        return 0
    finally:
        conn.close()

def cmd_fragments_export(args: argparse.Namespace) -> int:
    source_id = int(getattr(args, "source_id", 0) or 0)
    if not source_id:
        print("ERROR: --source-id is required", file=sys.stderr)
        return 2

    state = (getattr(args, "state", "") or "").strip()
    needs_rewrite_only = bool(getattr(args, "needs_rewrite_only", False))
    bot_ready_only = bool(getattr(args, "bot_ready_only", False))
    q = (getattr(args, "q", "") or "").strip()
    limit = int(getattr(args, "limit", 0) or 0)

    out_path_raw = (getattr(args, "out", "") or "").strip()
    out_path = Path(out_path_raw) if out_path_raw else None
    include_meta = bool(getattr(args, "include_meta", True))
    include_text = bool(getattr(args, "include_text", True))

    conn = _connect(_staging_db_path())
    try:
        _ensure_staging_schema(conn)

        where = ["source_id=?"]
        params: list[Any] = [source_id]
        if state:
            where.append("state=?")
            params.append(state)
        if q:
            where.append("(key LIKE ? OR text LIKE ?)")
            like = f"%{q}%"
            params.extend([like, like])

        sql = f"""
        SELECT id, key, locale, topic_category, tone, abstraction_level, state, created_at, updated_at, meta_json, text
        FROM kb_fragments
        WHERE {' AND '.join(where)}
        ORDER BY id ASC
        """.strip()

        rows = conn.execute(sql, params).fetchall()

        # Apply derived filters in Python (meta_json + direct address)
        items: list[dict[str, Any]] = []
        skipped_filter = 0

        for r in rows:
            meta_obj = _json_load_dict(r["meta_json"])
            requires_rewrite = bool(meta_obj.get("requires_rewrite", False))
            txt = str(r["text"] or "")
            bot_ready = bool(_DIRECT_ADDRESS_RE.search(txt))

            if needs_rewrite_only and not requires_rewrite:
                skipped_filter += 1
                continue
            if bot_ready_only and not bot_ready:
                skipped_filter += 1
                continue

            obj: dict[str, Any] = {
                "id": int(r["id"]),
                "key": str(r["key"]),
                "locale": str(r["locale"]),
                "state": str(r["state"]),
                "topic_category": str(r["topic_category"]),
                "tone": str(r["tone"]),
                "abstraction_level": str(r["abstraction_level"]),
                "created_at": str(r["created_at"]),
                "updated_at": str(r["updated_at"]),
                "requires_rewrite": requires_rewrite,
                "bot_ready": bot_ready,
            }
            if include_text:
                obj["text"] = txt
            if include_meta:
                obj["meta_json"] = meta_obj

            items.append(obj)
            if limit and len(items) >= limit:
                break

        # Write JSONL
        out_fp = None
        try:
            if out_path:
                out_path.parent.mkdir(parents=True, exist_ok=True)
                out_fp = out_path.open("w", encoding="utf-8", newline="\n")
                writer = out_fp
            else:
                writer = sys.stdout

            for obj in items:
                writer.write(json.dumps(obj, ensure_ascii=False) + "\n")

            if out_fp:
                out_fp.flush()
        finally:
            if out_fp:
                out_fp.close()

        print(
            f"export: source_id={source_id} matched={len(rows)} exported={len(items)} skipped_filter={skipped_filter}"
            + (f" out={str(out_path)}" if out_path else ""),
            file=sys.stderr,
        )
        return 0
    finally:
        conn.close()

def cmd_fragments_purge(args: argparse.Namespace) -> int:
    source_id = int(getattr(args, "source_id", 0) or 0)
    if not source_id:
        print("ERROR: --source-id is required", file=sys.stderr)
        return 2

    only_extractor = (getattr(args, "only_extractor", "") or "").strip()
    state = (getattr(args, "state", "") or "").strip()
    limit = int(getattr(args, "limit", 0) or 0)

    dry_run = bool(getattr(args, "dry_run", False))
    as_json = bool(getattr(args, "json", False))
    compact = bool(getattr(args, "compact", False))

    who = (getattr(args, "who", "KB") or "KB").strip()
    note = (getattr(args, "note", "purge") or "purge").strip()

    conn = _connect(_staging_db_path())
    try:
        _ensure_staging_schema(conn)

        where = ["source_id=?"]
        params: list[Any] = [source_id]
        if state:
            where.append("state=?")
            params.append(state)

        rows = conn.execute(
            f"""
            SELECT id, key, locale, state, meta_json
            FROM kb_fragments
            WHERE {' AND '.join(where)}
            ORDER BY id ASC
            """.strip(),
            params,
        ).fetchall()

        selected: list[int] = []
        skipped_filter = 0

        for r in rows:
            meta = _json_load_dict(r["meta_json"])
            extractor = str(meta.get("extractor") or "")
            if only_extractor and extractor != only_extractor:
                skipped_filter += 1
                continue

            selected.append(int(r["id"]))
            if limit and len(selected) >= limit:
                break

        deleted = 0
        now = _utcnow_iso()

        if not dry_run:
            for fid in selected:
                row = conn.execute("SELECT state FROM kb_fragments WHERE id=?", (fid,)).fetchone()
                from_state = str(row["state"] if row else "")
                conn.execute(
                    """
                    INSERT INTO kb_events(fragment_id, from_state, to_state, who, note, ts)
                    VALUES(?,?,?,?,?,?)
                    """.strip(),
                    (fid, from_state, "purged", who, note, now),
                )
                conn.execute("DELETE FROM kb_fragments WHERE id=?", (fid,))
                deleted += 1
            conn.commit()
        else:
            deleted = len(selected)

        payload = {
            "dry_run": dry_run,
            "source_id": source_id,
            "state": state or None,
            "only_extractor": only_extractor or None,
            "matched": len(rows),
            "selected": len(selected),
            "deleted": deleted,
            "skipped_filter": skipped_filter,
        }

        if as_json:
            print(json.dumps(payload, ensure_ascii=False, indent=None if compact else 2))
        else:
            print(
                f"purge: source_id={source_id} matched={len(rows)} selected={len(selected)} "
                f"deleted={deleted} skipped_filter={skipped_filter}"
                + (f" extractor={only_extractor}" if only_extractor else ""),
                file=sys.stderr,
            )
        return 0
    finally:
        conn.close()


def cmd_ingest(args: argparse.Namespace) -> int:
    path = Path(getattr(args, "path", "") or "")
    if not path.exists():
        print(f"ERROR: path not found: {path}", file=sys.stderr)
        return 2

    default_locale = (getattr(args, "locale", "") or "ru-RU").strip() or "ru-RU"
    chunk_size = int(getattr(args, "chunk_size", 6000) or 6000)
    dry_run = bool(getattr(args, "dry_run", False))
    as_json = bool(getattr(args, "json", False))
    compact = bool(getattr(args, "compact", False))
    force = bool(getattr(args, "force", False))

    files: list[Path] = []
    if path.is_dir():
        for p in sorted(path.rglob("*")):
            if p.is_file() and (p.suffix.lower() in {".fb2", ".zip"}):
                files.append(p)
    else:
        files = [path]

    conn = _connect(_staging_db_path())
    try:
        _ensure_staging_schema(conn)
        started_at = _utcnow_iso()

        sources_created = 0
        sources_reused = 0
        chunks_created = 0
        errors: list[dict[str, Any]] = []

        for fp in files:
            try:
                raw = _read_bytes(fp)
                fb2_bytes = raw
                source_type = "fb2"
                if fp.suffix.lower() == ".zip":
                    fb2_bytes = _extract_fb2_from_zip(fp)
                    source_type = "fb2.zip"

                sha = _sha256_bytes(fb2_bytes)
                file_size = fp.stat().st_size
                file_name = fp.name

                row = conn.execute("SELECT id FROM kb_sources WHERE sha256=?", (sha,)).fetchone()
                if row:
                    source_id = int(row["id"])
                    sources_reused += 1
                    if force and not dry_run:
                        conn.execute("DELETE FROM kb_source_chunks WHERE source_id=?", (source_id,))
                else:
                    if dry_run:
                        source_id = -1
                    else:
                        cur = conn.execute(
                            "INSERT INTO kb_sources(title,path,sha256,imported_at,notes,source_type,author,locale,file_name,file_size,meta_json) "
                            "VALUES(?,?,?,?,?,?,?,?,?,?,?)",
                            (
                                file_name,
                                str(fp),
                                sha,
                                _utcnow_iso(),
                                "",
                                source_type,
                                None,
                                default_locale,
                                file_name,
                                file_size,
                                "{}",
                            ),
                        )
                        source_id = int(cur.lastrowid)
                    sources_created += 1

                locale, plain = _fb2_to_text(fb2_bytes)
                locale = locale or default_locale
                chunks = _chunk_text(plain, chunk_size)

                if not chunks:
                    continue

                if dry_run:
                    chunks_created += len(chunks)
                    continue

                if row and not force:
                    existing = conn.execute(
                        "SELECT COUNT(1) AS n FROM kb_source_chunks WHERE source_id=?", (source_id,)
                    ).fetchone()
                    if existing and int(existing["n"] or 0) > 0:
                        continue

                for seq, (cs, ce, chunk_text) in enumerate(chunks, start=1):
                    ch_sha = _sha256_bytes(chunk_text.encode("utf-8"))
                    conn.execute(
                        "INSERT OR REPLACE INTO kb_source_chunks(source_id,seq,locale,text,sha256,char_start,char_end,created_at) "
                        "VALUES(?,?,?,?,?,?,?,?)",
                        (source_id, seq, locale, chunk_text, ch_sha, cs, ce, _utcnow_iso()),
                    )
                chunks_created += len(chunks)

                if row:
                    conn.execute(
                        "UPDATE kb_sources SET source_type=?, locale=?, file_name=?, file_size=? WHERE id=?",
                        (source_type, locale, file_name, file_size, source_id),
                    )
            except Exception as e:
                errors.append({"file": str(fp), "error": str(e)})

        finished_at = _utcnow_iso()
        if not dry_run:
            conn.execute(
                "INSERT INTO kb_ingest_runs(started_at,finished_at,input_path,tool,tool_version,files_total,"
                "sources_created,sources_reused,chunks_created,errors_json) VALUES(?,?,?,?,?,?,?,?,?,?)",
                (
                    started_at,
                    finished_at,
                    str(path),
                    "kb_ingest",
                    "v1",
                    len(files),
                    sources_created,
                    sources_reused,
                    chunks_created,
                    json.dumps({"errors": errors}, ensure_ascii=False),
                ),
            )
            conn.commit()

        report = {
            "dry_run": dry_run,
            "files_total": len(files),
            "sources_created": sources_created,
            "sources_reused": sources_reused,
            "chunks_created": chunks_created,
            "errors": errors,
        }
        if as_json:
            print(json.dumps(report, ensure_ascii=False, indent=None if compact else 2))
        else:
            print(
                f"ingest: files={len(files)} sources_created={sources_created} "
                f"sources_reused={sources_reused} chunks={chunks_created}",
                file=sys.stderr,
            )
        return 0 if not errors else 1
    finally:
        conn.close()


def _atomize_chunk(
    text: str, *, min_len: int, max_len: int, target_min: int, target_max: int, max_newlines: int
) -> list[tuple[int, int, str]]:
    text = _normalize_text(text)
    if not text:
        return []

    paras = [p.strip() for p in text.split("\n\n") if p.strip()]
    atoms: list[tuple[int, int, str]] = []
    buf: list[str] = []
    start = 0
    pos = 0

    def flush(end_pos: int) -> None:
        nonlocal buf, start
        if not buf:
            return
        atom = "\n\n".join(buf).strip()
        atom = _normalize_text(atom)
        if not atom:
            buf = []
            return
        atom = re.sub(r"\n{3,}", "\n\n", atom)
        if atom.count("\n") > max_newlines:
            atom = atom.replace("\n\n", " ").replace("\n", " ")
            atom = re.sub(r"\s{2,}", " ", atom).strip()

        if len(atom) < min_len or len(atom) > max_len:
            buf = []
            return
        low = atom.lower()
        if "http://" in low or "https://" in low or "```" in atom:
            buf = []
            return
        if not re.search(r"[.!?‚Ä¶]", atom):
            buf = []
            return

        atoms.append((start, end_pos, atom))
        buf = []

    for p in paras:
        if not buf:
            start = pos
        projected = sum(len(x) for x in buf) + (2 * len(buf)) + len(p)
        if projected > target_max and buf:
            flush(pos)
            start = pos
        buf.append(p)
        pos += len(p) + 2

        cur_len = sum(len(x) for x in buf) + (2 * (len(buf) - 1) if len(buf) > 1 else 0)
        if cur_len >= target_min:
            flush(pos)

    if buf:
        flush(pos)
    return atoms


def cmd_atomize(args: argparse.Namespace) -> int:
    source_id = int(getattr(args, "source_id", 0) or 0)
    do_all = bool(getattr(args, "all", False))
    mode = (getattr(args, "mode", "b") or "b").strip().lower()
    state = (getattr(args, "state", "needs_review") or "needs_review").strip()
    locale_override = (getattr(args, "locale", "") or "").strip()
    dry_run = bool(getattr(args, "dry_run", False))
    as_json = bool(getattr(args, "json", False))
    compact = bool(getattr(args, "compact", False))
    limit = int(getattr(args, "limit", 0) or 0)

    min_len = int(getattr(args, "min_len", 120) or 120)
    max_len = int(getattr(args, "max_len", 1800) or 1800)
    target_min = int(getattr(args, "target_min", 450) or 450)
    target_max = int(getattr(args, "target_max", 900) or 900)
    max_newlines = int(getattr(args, "max_newlines", 6) or 6)

    if mode not in {"a", "b"}:
        print("ERROR: --mode must be 'a' or 'b'", file=sys.stderr)
        return 2

    conn = _connect(_staging_db_path())
    try:
        _ensure_staging_schema(conn)

        where = []
        params: list[Any] = []
        if do_all:
            where.append("1=1")
        else:
            where.append("source_id=?")
            params.append(source_id)

        sql = f"SELECT id, source_id, seq, locale, text FROM kb_source_chunks WHERE {' AND '.join(where)} ORDER BY source_id, seq"
        rows = conn.execute(sql, params).fetchall()

        inserted = 0
        skipped_existing = 0
        skipped_mode = 0

        for r in rows:
            chunk_id = int(r["id"])
            sid = int(r["source_id"])
            chunk_locale = (locale_override or r["locale"] or "ru-RU")
            chunk_text = r["text"] or ""
            atoms = _atomize_chunk(
                chunk_text,
                min_len=min_len,
                max_len=max_len,
                target_min=target_min,
                target_max=target_max,
                max_newlines=max_newlines,
            )
            for (cs, ce, atom_text) in atoms:
                has_addr = bool(_DIRECT_ADDRESS_RE.search(atom_text))
                if mode == "a" and not has_addr:
                    skipped_mode += 1
                    continue

                content_hash = _sha1_text(_normalize_text(atom_text))
                key = f"src{sid}.{chunk_locale}.{content_hash[:12]}"

                exists = conn.execute(
                    "SELECT id FROM kb_fragments WHERE key=? AND locale=?",
                    (key, chunk_locale),
                ).fetchone()
                if exists:
                    skipped_existing += 1
                    continue

                meta = {
                    "source_id": sid,
                    "chunk_id": chunk_id,
                    "chunk_seq": int(r["seq"]),
                    "char_start": int(cs),
                    "char_end": int(ce),
                    "extractor": "atomizer_v1",
                    "content_hash": content_hash,
                    "requires_rewrite": (mode == "b" and not has_addr),
                }

                if dry_run:
                    inserted += 1
                    continue

                conn.execute(
                    """
                    INSERT INTO kb_fragments(
                      key, locale, topic_category, text, tone, abstraction_level, state,
                      factors_json, meta_json, source_id, author, created_at, updated_at
                    ) VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?)
                    """,
                    (
                        key,
                        chunk_locale,
                        getattr(args, "topic", "book") or "book",
                        atom_text,
                        getattr(args, "tone", "neutral") or "neutral",
                        getattr(args, "abstraction", "psychological") or "psychological",
                        state,
                        "{}",
                        json.dumps(meta, ensure_ascii=False),
                        sid,
                        "INGEST",
                        _utcnow_iso(),
                        _utcnow_iso(),
                    ),
                )
                inserted += 1
                if limit and inserted >= limit:
                    break
            if limit and inserted >= limit:
                break

        if not dry_run:
            conn.commit()

        report = {
            "dry_run": dry_run,
            "mode": mode,
            "inserted": inserted,
            "created": inserted,
            "skipped_existing": skipped_existing,
            "skipped_mode": skipped_mode,
        }
        if as_json:
            print(json.dumps(report, ensure_ascii=False, indent=None if compact else 2))
        else:
            print(
                f"atomize: inserted={inserted} skipped_existing={skipped_existing} skipped_mode={skipped_mode}",
                file=sys.stderr,
            )
        return 0
    finally:
        conn.close()


# ----------------------------
# Argparse
# ----------------------------


def build_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(prog="kb")
    sub = p.add_subparsers(dest="cmd", required=True)

    sp = sub.add_parser("init", help="init staging.db schema")
    sp.set_defaults(func=cmd_init)

    sp = sub.add_parser("build", help="build production knowledge.db from enabled staging fragments")
    sp.add_argument("--dry-run", action="store_true", help="show what would change, but do not write to production DB")
    sp.add_argument("--json", action="store_true", help="print JSON report (works with --dry-run too)")
    sp.add_argument("--compact", action="store_true", help="with --json: print one-line JSON (no indent)")
    sp.set_defaults(func=cmd_build)

    sp = sub.add_parser("add", help="add fragment to staging")
    sp.add_argument("--key", required=True)
    sp.add_argument("--locale", default="ru-RU")
    sp.add_argument("--topic", required=True)
    sp.add_argument("--tone", required=True, choices=["supportive", "neutral", "warning"])
    sp.add_argument("--abstraction", required=True, choices=["psychological", "symbolic", "behavioral"])
    sp.add_argument(
        "--state",
        default="draft",
        choices=["draft", "needs_review", "reviewed", "annotated", "validated", "enabled", "archived"],
    )
    sp.add_argument("--text", required=True)
    sp.add_argument("--author", default="KB")
    sp.add_argument("--who", default="KB")
    sp.add_argument("--factors-json", dest="factors_json", default="{}")
    sp.add_argument("--meta-json", dest="meta_json", default="{}")
    sp.add_argument("--source-title", dest="source_title", default="")
    sp.add_argument("--source-path", dest="source_path", default="")
    sp.add_argument("--source-notes", dest="source_notes", default="")
    sp.set_defaults(func=cmd_add)

    sp = sub.add_parser("state", help="change fragment state and write kb_events")
    sp.add_argument("--key", required=True)
    sp.add_argument("--locale", default="ru-RU")
    sp.add_argument(
        "--to",
        required=True,
        choices=["draft", "needs_review", "reviewed", "annotated", "validated", "enabled", "archived"],
    )
    sp.add_argument("--who", default="KB")
    sp.add_argument("--note", default="")
    sp.set_defaults(func=cmd_state)

    sp = sub.add_parser("validate", help="validate staging fragments and move to state=validated")
    sp.add_argument("--key", default="", help="fragment key (optional)")
    sp.add_argument("--verbose", action="store_true")
    sp.add_argument("--locale", default=None)
    sp.add_argument("--state", default=None)
    sp.add_argument("--limit", type=int, default=1000)
    sp.add_argument("--who", default="KB")
    sp.add_argument("--note", default="validated")
    sp.add_argument("--strict", action="store_true")
    sp.add_argument("--recheck", action="store_true")
    sp.add_argument("--json", action="store_true")
    sp.set_defaults(func=cmd_validate)

    sp = sub.add_parser("list", help="list fragments in staging (default) or items in production (--prod)")
    sp.add_argument("--prod", action="store_true")
    sp.add_argument("--active", action="store_true")
    sp.add_argument("--inactive", action="store_true")
    sp.add_argument("--with-text", action="store_true")
    sp.add_argument("--state", default="")
    sp.add_argument("--source-id", type=int, default=0)
    sp.add_argument("--needs-rewrite-only", dest="needs_rewrite_only", action="store_true",
                    help="(staging) only fragments with meta.requires_rewrite")
    sp.add_argument("--bot-ready-only", dest="bot_ready_only", action="store_true",
                    help="(staging) only fragments that already contain direct address")
    sp.add_argument("--topic", default="")
    sp.add_argument("--locale", default="")
    sp.add_argument("--key", default="", help="exact key match")
    sp.add_argument("--q", default="", help="search in key/text (LIKE)")
    sp.add_argument("--limit", type=int, default=50)
    sp.set_defaults(func=cmd_list)

    sp = sub.add_parser("show", help="show a key in staging + production")
    sp.add_argument("--key", required=True)
    sp.add_argument("--with-text", action="store_true")
    sp.add_argument("--json", action="store_true")
    sp.set_defaults(func=cmd_show)

    sp = sub.add_parser("restore", help="restore a key to enabled (archived -> needs_review -> validated -> enabled)")
    sp.add_argument("--key", required=True)
    sp.add_argument("--locale", default=None)
    sp.add_argument("--review-state", dest="review_state", default="needs_review")
    sp.add_argument("--who", default="KB")
    sp.add_argument("--note", default="restore")
    sp.add_argument("--dry-run", action="store_true")
    sp.add_argument("--build", action="store_true")
    sp.add_argument("--json", action="store_true")
    sp.add_argument("--compact", action="store_true")
    sp.add_argument("--verbose", action="store_true")
    sp.set_defaults(func=cmd_restore)

    # ---- ingest / atomize / sources / fragments ----
    sp = sub.add_parser("ingest", help="ingest .fb2/.zip books into staging sources + chunks")
    sp.add_argument("--path", required=True)
    sp.add_argument("--locale", default="ru-RU")
    sp.add_argument("--chunk-size", dest="chunk_size", type=int, default=6000)
    sp.add_argument("--force", action="store_true")
    sp.add_argument("--dry-run", action="store_true")
    sp.add_argument("--json", action="store_true")
    sp.add_argument("--compact", action="store_true")
    sp.set_defaults(func=cmd_ingest)

    sp = sub.add_parser("atomize", help="create candidate kb_fragments from ingested chunks")
    sp.add_argument("--source-id", dest="source_id", type=int, default=0)
    sp.add_argument("--all", action="store_true")
    sp.add_argument("--mode", choices=["a", "b"], default="b")
    sp.add_argument(
        "--state",
        default="needs_review",
        choices=["draft", "needs_review", "reviewed", "annotated", "validated", "enabled", "archived"],
    )
    sp.add_argument("--locale", default="")
    sp.add_argument("--topic", default="book")
    sp.add_argument("--tone", default="neutral", choices=["supportive", "neutral", "warning"])
    sp.add_argument("--abstraction", default="psychological", choices=["psychological", "symbolic", "behavioral"])
    sp.add_argument("--min-len", dest="min_len", type=int, default=120)
    sp.add_argument("--max-len", dest="max_len", type=int, default=1800)
    sp.add_argument("--target-min", dest="target_min", type=int, default=450)
    sp.add_argument("--target-max", dest="target_max", type=int, default=900)
    sp.add_argument("--max-newlines", dest="max_newlines", type=int, default=6)
    sp.add_argument("--limit", type=int, default=0)
    sp.add_argument("--dry-run", action="store_true")
    sp.add_argument("--json", action="store_true")
    sp.add_argument("--compact", action="store_true")
    sp.set_defaults(func=cmd_atomize)

    sp = sub.add_parser("sources", help="sources helpers")
    sub2 = sp.add_subparsers(dest="sources_cmd", required=True)
    sp2 = sub2.add_parser("list", help="list ingested sources")
    spx = sub2.add_parser("diagnose", help="diagnose text garbling for a source (chunks)")
    spx.add_argument("--source-id", type=int, required=True)
    spx.add_argument("--threshold", type=float, default=0.01)
    spx.add_argument("--top", type=int, default=10)
    spx.add_argument("--json", action="store_true")
    spx.add_argument("--compact", action="store_true")
    spx.set_defaults(func=cmd_sources_diagnose)
    sp2.add_argument("--q", default="")
    sp2.add_argument("--limit", type=int, default=200)
    sp2.add_argument("--json", action="store_true")
    sp2.add_argument("--compact", action="store_true")
    sp2.set_defaults(func=cmd_sources_list)

    sp = sub.add_parser("fragments", help="fragments helpers (alias around 'list' for staging)")
    sub3 = sp.add_subparsers(dest="fragments_cmd", required=True)
    sp3 = sub3.add_parser("list", help="list staging fragments (supports --source-id)")
    sp3.add_argument("--state", default="")
    sp3.add_argument("--source-id", type=int, default=0)
    sp3.add_argument("--needs-rewrite-only", dest="needs_rewrite_only", action="store_true")
    sp3.add_argument("--bot-ready-only", dest="bot_ready_only", action="store_true")
    sp3.add_argument("--topic", default="")
    sp3.add_argument("--locale", default="")
    sp3.add_argument("--key", default="")
    sp3.add_argument("--q", default="")
    sp3.add_argument("--limit", type=int, default=50)
    
    sp3.set_defaults(func=cmd_list)
    sp4 = sub3.add_parser("sample", help="print sample fragments for review")
    sp4.add_argument("--source-id", type=int, required=True)
    sp4.add_argument("--state", default="")
    sp4.add_argument("--limit", type=int, default=20)
    sp4.add_argument("--with-text", action="store_true")
    sp4.add_argument("--needs-rewrite-only", dest="needs_rewrite_only", action="store_true")
    sp4.add_argument("--bot-ready-only", dest="bot_ready_only", action="store_true")
    sp4.add_argument("--json", action="store_true")
    sp4.add_argument("--compact", action="store_true")
    sp4.set_defaults(func=cmd_fragments_sample)

    sp5 = sub3.add_parser("stats", help="stats for fragments of a source")
    sp5.add_argument("--source-id", type=int, required=True)
    sp5.add_argument("--state", default="")
    sp5.add_argument("--json", action="store_true")
    sp5.add_argument("--compact", action="store_true")
    sp5.set_defaults(func=cmd_fragments_stats)

    sp6 = sub3.add_parser("promote", help="bulk move fragments between states (e.g., needs_review -> reviewed)")
    sp6.add_argument("--source-id", type=int, required=True)
    sp6.add_argument("--from", dest="from_state", default="needs_review")
    sp6.add_argument("--to", dest="to_state", default="reviewed")
    sp6.add_argument("--only-bot-ready", dest="only_bot_ready", action="store_true")
    sp6.add_argument("--only-needs-rewrite", dest="only_needs_rewrite", action="store_true")
    sp6.add_argument("--limit", type=int, default=0)
    sp6.add_argument("--who", default="KB")
    sp6.add_argument("--note", default="promote")
    sp6.add_argument("--dry-run", action="store_true")
    sp6.add_argument("--json", action="store_true")
    sp6.add_argument("--compact", action="store_true")
    sp6.set_defaults(func=cmd_fragments_promote)

    sp7 = sub3.add_parser("export", help="export fragments to JSONL for bulk editing")
    sp7.add_argument("--source-id", type=int, required=True)
    sp7.add_argument("--state", default="")
    sp7.add_argument("--q", default="")
    sp7.add_argument("--needs-rewrite-only", dest="needs_rewrite_only", action="store_true")
    sp7.add_argument("--bot-ready-only", dest="bot_ready_only", action="store_true")
    sp7.add_argument("--limit", type=int, default=0)
    sp7.add_argument("--out", default="", help="write to file (UTF-8). If omitted, writes to stdout")
    sp7.add_argument("--include-meta", dest="include_meta", action="store_true", default=True)
    sp7.add_argument("--no-meta", dest="include_meta", action="store_false")
    sp7.add_argument("--include-text", dest="include_text", action="store_true", default=True)
    sp7.add_argument("--no-text", dest="include_text", action="store_false")
    sp7.set_defaults(func=cmd_fragments_export)

    sp8 = sub3.add_parser("purge", help="delete staging fragments by source (optionally by extractor)")
    sp8.add_argument("--source-id", type=int, required=True)
    sp8.add_argument("--state", default="")
    sp8.add_argument("--only-extractor", dest="only_extractor", default="")
    sp8.add_argument("--limit", type=int, default=0)
    sp8.add_argument("--who", default="KB")
    sp8.add_argument("--note", default="purge")
    sp8.add_argument("--dry-run", action="store_true")
    sp8.add_argument("--json", action="store_true")
    sp8.add_argument("--compact", action="store_true")
    sp8.set_defaults(func=cmd_fragments_purge)


    return p


def main(argv: Optional[list[str]] = None) -> int:
    parser = build_parser()
    args = parser.parse_args(argv)
    return int(args.func(args) or 0)


if __name__ == "__main__":
    raise SystemExit(main())
